{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"><font color=\"00308F\" size=110>Numerical Differentiation: Finite Difference Method</font></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using finite differences to approximate derivatives\n",
    "\n",
    "The material in this notebook is a supplement to lesson 3 course notes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that you're given a table of values of some \"secret\" function $f(x)$, but you are not given the function itself.  For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_values = (0,1,2,3,4)\n",
    "y_values = (0,1,8,27,64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to understand the function that generated these values, we could plot them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(x_values,y_values,'-o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the plot, the points have been connected by a line, and you might use the slope of this line if you wanted to estimate the derivative of the function.  For instance, to estimate the slope at $x=2$, you could use the slope of the line between $x=2$ and $x=3$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(x_values,y_values,'-o')\n",
    "plt.plot(x_values[2:4],y_values[2:4],'-r',linewidth=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\left. f'(x) \\right|_{x=2} \\approx \\frac{f(3)-f(2)}{3-2} = 19.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you might use the slope of the line between $x=1$ and $x=2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(x_values,y_values,'-o')\n",
    "plt.plot(x_values[1:3],y_values[1:3],'-r',linewidth=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\left. f'(x) \\right|_{x=2} \\approx \\frac{f(2)-f(1)}{2-1} = 7.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the derivative is defined as the limit of a formula rather like our first approximation above (referred to as a *forward difference*):\n",
    "\n",
    "$$ f'(x) = \\lim_{h\\to 0} \\frac{f(x+h)-f(x)}{(x+h)-x} = \\lim_{h\\to 0} \\frac{f(x+h)-f(x)}{h}.$$\n",
    "\n",
    "Notice that the fraction in the definition is just the slope of the line connecting the values of $f$ at $x$ and $x+h$.  We could equally well define the derivative in terms of a *backward difference*, similar to our second formula above:\n",
    "\n",
    "$$ f'(x) = \\lim_{h\\to 0} \\frac{f(x)-f(x-h)}{x-(x-h)} = \\lim_{h\\to 0} \\frac{f(x)-f(x-h)}{h}.$$\n",
    "\n",
    "Yet another way of defining the derivative would be to use the slope of the line connecting points $x+h$ and $x-h$, leading to\n",
    "\n",
    "$$ f'(x) = \\lim_{h \\to 0} \\frac{f(x+h) - f(x-h)}{2h}.$$\n",
    "\n",
    "Here the factor $2h$ in the denominator is just the length of the interval $(x-h,x+h)$.  Using this formula with our values gives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(x_values,y_values,'-ok')\n",
    "plt.plot((x_values[1],x_values[3]),(y_values[1],y_values[3]),'-r',linewidth=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\left. f'(x) \\right|_{x=2} \\approx \\frac{f(3)-f(1)}{3-1} = 13.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we only know the values of a function at a finite set of points, we can't compute these limits, so it makes sense to use a finite value of $h$ in order to approximate the derivative.  This approach is known as the *finite difference method*.  Soon we will use it to solve differential equations, but first let's examine its effectiveness in approximating derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have guessed that the \"secret\" function that generated these values is $f(x)=x^3$.  Let's compare our three estimates of the derivative with the true derivative: $f'(2) = 3\\cdot 2^2 = 12$.  Clearly the third formula (known as a *centered difference*) is the best approximation.  We can also see this by plotting the approximations and the tangent line (whose slope is that of the true derivative):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x**3\n",
    "\n",
    "x = np.linspace(0,4)\n",
    "tangent = 8 + 12*(x-2)  #Take derivative of f @2 and plug into y-y1=m(x-x1)\n",
    "plot(x,f(x),'k',linewidth=2)\n",
    "plt.plot(x_values[2:4],y_values[2:4],'-g',linewidth=3)\n",
    "plt.plot(x_values[1:3],y_values[1:3],'-b',linewidth=3)\n",
    "plt.plot((x_values[1],x_values[3]),(y_values[1],y_values[3]),'-r',linewidth=3)\n",
    "plt.plot(x,tangent,'--k',linewidth=3)\n",
    "plt.legend(['f(x)','Forward','Backward','Centered','Tangent'],loc='best')\n",
    "plt.axis((0.5,3.5,0,40))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's clear that the centered difference approximation (slope of the red line) is closest to the derivative (slope of the dotted line).  Would this be true if we picked another function $f(x)$, or if we used function values at different points?  Let's answer the second question with an experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#! Define a function to calculate the forward difference using a second-order Taylor expansion \n",
    "#! (Hint: the formula is given above.)\n",
    "def forward_diff(x, h):\n",
    "    \n",
    "    return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the point of interest and the analytic slope\n",
    "x = 2.\n",
    "df = 12.\n",
    "\n",
    "# Investigate the impact of the step sizes on the accuracy\n",
    "for h in (0.1, 0.05, 0.025):\n",
    "    forward = forward_diff(x, h)\n",
    "    forward_error = forward - df\n",
    "    print forward_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems clear from the results that reducing $h$ by a factor of two also reduces the error in the forward difference approximation by a factor of two.\n",
    "\n",
    "**Now add backward and centered difference approximations. Try to figure out what happens to their errors as $h$ decreases.**  Is the centered difference always the most accurate?  Do you think it will still be the most accurate if we continue reducing $h$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#! Add backward diff and central diff methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#! Investigate the impact of the step sizes on the accuracy for all three\n",
    "print \"Forward     Backwards     Central\"\n",
    "print \"=================================\"\n",
    "for h in (0.1, 0.05, 0.025):\n",
    "    \n",
    "    print '{:3.6f}    {:3.6f}    {:3.6f}'.format(forward - df, backward - df, central - df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating truncation errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error made by a finite difference approximation is called *truncation error*.  Why?  Well, we can estimate the error by expanding each function value in a *Taylor series*:\n",
    "\n",
    "$$f(x+h) = f(x) + h f'(x) + \\frac{1}{2}h^2 f''(x) + \\frac{1}{6} h^3 f'''(x) + {\\mathcal O}(h^4)$$\n",
    "\n",
    "Here ${\\mathcal O}(h^4)$ indicates that the rest of the terms in the series vanish at least as quickly as $h^4$ when $h\\to 0$.\n",
    "Substituting this series in our forward difference formula gives\n",
    "\\begin{align}\n",
    "\\frac{f(x+h) - f(x)}{h} & = \\frac{f(x) + h f'(x) + \\frac{1}{2}h^2 f''(x) + \\frac{1}{6} h^3 f'''(x) + {\\mathcal O}(h^4) - f(x)}{h} \\\\\n",
    "& = f'(x) + \\frac{1}{2}h f''(x) + \\frac{1}{6} h^2 f'''(x) + {\\mathcal O}(h^3) \\\\\n",
    "& = f'(x) + \\frac{1}{2}h f''(x) + {\\mathcal O}(h^2) \\\\\n",
    "& = f'(x) + {\\mathcal O}(h).\n",
    "\\end{align}\n",
    "\n",
    "This analysis confirms our intuition that the forward difference approximates $f'(x)$, but it tells us much more.  Most importantly, it shows that the largest term in the error in this approximation is proportional to $h$.  That's why we saw that decreasing $h$ by a factor of two caused the error to decrease by the same amount.\n",
    "\n",
    "Notice that if we truncated the Taylor series after the first term, we would get the forward difference formula exactly.  That is why the error is referred to as *truncation error* -- it's the error we get from truncating an infinite series.  The term $\\frac{1}{2} h f''(x)$ is referred to as the *leading truncation error* because when $h$ is very small we expect that term to be much bigger than all the ones that come after it.\n",
    "\n",
    "Since in our simple example we know the function $f(x)$, we could evaluate error terms in the series above to get a better approximation of the error.  But typically we won't know what the function is (if we did, why would we need finite differences?), so we'll be most interested in knowing what power of $h$ multiplies the leading error term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differentiation with Numpy\n",
    "\n",
    "As expected, there are Python modules that can do this for you!  For this, let's look at the [gradient](https://docs.scipy.org/doc/numpy-1.9.2/reference/generated/numpy.gradient.html#numpy.gradient) method in numpy.\n",
    "\n",
    "**NOTE: While Python may make it easy, AFIT can make it hard.  The current numpy version is 1.13.1 while AFIT's current version is 1.9.2 as of 4 October, 2017.  For this particular method, many changes have been made with the updates.  The manual I linked to is for the correct version of the gradient method.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print np.__version__\n",
    "\n",
    "data = np.array([1, 8, 27, 64], dtype=np.float)\n",
    "\n",
    "#! Compute the derivative at each point using numpy's gradient method\n",
    "\n",
    "\n",
    "#! What if you had data at increments of 0.5?\n",
    "data = np.array([0.125, 1, 3.375, 8, 15.625, 27, 42.875, 64], dtype=np.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
